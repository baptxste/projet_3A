{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import torch.nn as nn \n",
    "from torch import optim\n",
    "# from sklearn.decomposition import TruncatedSVD as svds\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.preprocessing import normalize\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from spice import SpiceEmbeddingModel\n",
    "from gru import GRUEncoder, GRUDecoder\n",
    "from matplotlib import pyplot as plt\n",
    "from time import time \n",
    "import spice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spice encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "URL = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "FILE_PATH = \"shakespeare.txt\"\n",
    "EMB_DIM = 64\n",
    "WINDOW = 5\n",
    "\n",
    "\n",
    "try:\n",
    "    with open(FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "except FileNotFoundError:\n",
    "    response = requests.get(URL)\n",
    "    text = response.text\n",
    "    with open(FILE_PATH, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "\n",
    "\n",
    "# spice_model = SpiceEmbeddingModel(emb_dim=50, window_size=3)\n",
    "\n",
    "# dataset = spice_model.get_dataset(text)\n",
    "# # spice_model.save_model(spice_model.embeddings)\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "# for batch in dataloader:\n",
    "#     sentence_embeddings = batch\n",
    "#     print(\"Sentence Embeddings: \", len(sentence_embeddings[0]))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.011461593141906302\n",
      "Epoch 2/50, Loss: 0.01136212462337095\n",
      "Epoch 3/50, Loss: 0.011339516691143879\n",
      "Epoch 4/50, Loss: 0.01135277928551659\n",
      "Epoch 5/50, Loss: 0.01133617614819245\n",
      "Epoch 6/50, Loss: 0.01136466958136721\n",
      "Epoch 7/50, Loss: 0.01134530363329263\n",
      "Epoch 8/50, Loss: 0.011413543905787677\n",
      "Epoch 9/50, Loss: 0.011335558985592797\n",
      "Epoch 10/50, Loss: 0.011345485265006904\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 56\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Remettre les séquences dans l'ordre original\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# _, reverse_idx = perm_idx.sort()\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# reconstructed = reconstructed[reverse_idx]\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# fillers = fillers[reverse_idx]\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Calcul de la perte (ignorer le padding)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(reconstructed, fillers)\n\u001b[0;32m---> 56\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     59\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/micromamba/envs/cours2/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/cours2/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/cours2/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_size = 50 # taille des embeddings\n",
    "hidden_size = 128\n",
    "lr = 0.001\n",
    "num_layer=5\n",
    "num_epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     \"\"\" permet de rajouter du padding et renvoyer la taille de la séquence dans un batch\"\"\"\n",
    "#     batch = [item for item in batch if len(item) > 0]  # Filtrer les séquences vides\n",
    "#     lengths = torch.tensor([len(seq) for seq in batch])  # Longueurs originales\n",
    "#     padded_batch = pad_sequence(batch, batch_first=True, padding_value=0.0)  # Padding des séquences\n",
    "#     return padded_batch, lengths\n",
    "\n",
    "\n",
    "spice_model = SpiceEmbeddingModel()\n",
    "# spice_model.load_model()\n",
    "dataset = spice_model.get_dataset(text)\n",
    "# dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=spice.collate_fn_fillers_roles)\n",
    "\n",
    "\n",
    "\n",
    "encoder = GRUEncoder(input_size, hidden_size, num_layer).to(device)\n",
    "decoder = GRUDecoder(hidden_size, input_size, num_layer).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(list(encoder.parameters()) +list(decoder.parameters()), lr=lr)\n",
    "\n",
    "losses = []\n",
    "t0 = time()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # for batch, lengths in dataloader:\n",
    "    #     batch = batch.to(device)\n",
    "    for fillers, roles, lengths in dataloader:\n",
    "        fillers = fillers.to(device)  # (batch, seq_len, input_size)\n",
    "        # roles = roles.to(device)        # (batch, seq_len, role_dim)\n",
    "        lengths = lengths.to(device)    # (batch,)\n",
    "        # lengths = lengths.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        encoded, perm_idx = encoder(fillers, lengths)\n",
    "        reconstructed = decoder(encoded, lengths)\n",
    "        \n",
    "        # Remettre les séquences dans l'ordre original\n",
    "        # _, reverse_idx = perm_idx.sort()\n",
    "        # reconstructed = reconstructed[reverse_idx]\n",
    "        # fillers = fillers[reverse_idx]\n",
    "\n",
    "        # Calcul de la perte (ignorer le padding)\n",
    "        loss = criterion(reconstructed, fillers)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss/ len(dataloader))\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(dataloader)}\")\n",
    "t1 = time()\n",
    "torch.save(encoder, \"encoder.pth\")\n",
    "torch.save(decoder, \"decoder.pth\")\n",
    "print( f\"training time : {t1-t0}\")\n",
    "plt.plot(losses)\n",
    "plt.title(\"MSE over epoch\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "originale**: my lord you shall oerrule my mind for once\n",
      "reconstruite**: and to to and and to and and to\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def decode_sequence(spice_model, sequence):\n",
    "    \"\"\"decode les embeddings en mots\"\"\"\n",
    "    decoded_words = []\n",
    "    for embedding in sequence.cpu():\n",
    "        if embedding.sum() != 0:  # Ignorer le padding\n",
    "            decoded_words.append(spice_model.decode_embedding(embedding.detach().numpy(), top_n=1)[0])\n",
    "    return \" \".join(decoded_words)\n",
    "\n",
    "fillers, roles, lengths = next(iter(dataloader))  \n",
    "idx = random.randint(0, fillers.size(0) - 1)  #  phrase au hasard\n",
    "input_seq = fillers[idx].unsqueeze(0).to(device)  # le tenseur avec du padding\n",
    "mask = (input_seq != 0).any(dim=2)  # vérifie si chaque ligne contient des valeurs non nulles\n",
    "tensor_clean = input_seq[:, mask[0], :] # enlève les vecteurs nuls du padding\n",
    "length = torch.tensor([tensor_clean.shape[1]]).to(device)  # récupère la longueur originale sans le padding\n",
    "\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "with torch.no_grad():\n",
    "    encoded, _ = encoder(input_seq, length)\n",
    "    reconstructed_seq = decoder(encoded, length)\n",
    "\n",
    "\n",
    "original_text = decode_sequence(spice_model, input_seq.squeeze(0))\n",
    "reconstructed_text = decode_sequence(spice_model, reconstructed_seq.squeeze(0))\n",
    "\n",
    "\n",
    "print(f\"originale**: {original_text}\")\n",
    "print(f\"reconstruite**: {reconstructed_text}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
