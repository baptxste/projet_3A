{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.preprocessing import normalize\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "URL = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "FILE_PATH = \"shakespeare.txt\"\n",
    "EMB_DIM = 64\n",
    "WINDOW = 5\n",
    "\n",
    "\n",
    "try:\n",
    "    with open(FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "except FileNotFoundError:\n",
    "    response = requests.get(URL)\n",
    "    text = response.text\n",
    "    with open(FILE_PATH, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, sentence_tensor):\n",
    "        self.sentence_tensor = sentence_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentence_tensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentence_tensor[idx]\n",
    "\n",
    "class SpiceEmbeddingModel:\n",
    "    def __init__(self, emb_dim=50, window_size=3):\n",
    "        self.emb_dim = emb_dim\n",
    "        self.window_size = window_size\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.embeddings = None\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        sentences = text.split('\\n')\n",
    "        sentences = [lst for lst in sentences if \":\" not in lst]\n",
    "        text = '\\n'.join(sentences).lower()\n",
    "        text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "        sentences = [lst for lst in text.split('\\n') if lst]\n",
    "        return sentences\n",
    "\n",
    "    def build_cooccurrence_matrix(self, words):\n",
    "        if not self.word2idx:\n",
    "            vocab = sorted(set(words))\n",
    "            self.word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "            self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        \n",
    "        vocab_size = len(self.word2idx)\n",
    "        co_matrix = np.zeros((vocab_size, vocab_size))\n",
    "        \n",
    "        for i, word in enumerate(words):\n",
    "            word_idx = self.word2idx[word]\n",
    "            start, end = max(0, i - self.window_size), min(len(words), i + self.window_size + 1)\n",
    "            for j in range(start, end):\n",
    "                if i != j:\n",
    "                    neighbor_idx = self.word2idx[words[j]]\n",
    "                    co_matrix[word_idx, neighbor_idx] += 1\n",
    "        \n",
    "        return co_matrix\n",
    "\n",
    "    def spice_embedding(self, co_matrix):\n",
    "        u, s, vt = svds(co_matrix, k=self.emb_dim)\n",
    "        embeddings = normalize(u @ np.diag(np.sqrt(s)))\n",
    "        return embeddings\n",
    "\n",
    "    def save_model(self):\n",
    "        with open(\"spice_embeddings.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.embeddings, f)\n",
    "        with open(\"spice_vocab.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.word2idx, f)\n",
    "\n",
    "    def load_model(self):\n",
    "        with open(\"spice_embeddings.pkl\", \"rb\") as f:\n",
    "            self.embeddings = pickle.load(f)\n",
    "        with open(\"spice_vocab.pkl\", \"rb\") as f:\n",
    "            self.word2idx = pickle.load(f)\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "\n",
    "    def encode_word(self, word):\n",
    "        return self.embeddings[self.word2idx[word]] if word in self.word2idx else None\n",
    "\n",
    "    def decode_embedding(self, vector, top_n=5):\n",
    "        similarities = np.dot(self.embeddings, vector)\n",
    "        closest_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "        return [self.idx2word[idx] for idx in closest_indices]\n",
    "\n",
    "    def get_dataset(self, text):\n",
    "        sentences = self.preprocess_text(text)\n",
    "        words = [word for sentence in sentences for word in sentence.split()]\n",
    "        \n",
    "        co_matrix = self.build_cooccurrence_matrix(words)\n",
    "        self.embeddings = self.spice_embedding(co_matrix)\n",
    "        self.save_model()  \n",
    "\n",
    "        sentence_embeddings = [\n",
    "            torch.tensor([self.embeddings[self.word2idx[word]] for word in sentence.split() if word in self.word2idx], dtype=torch.float32)\n",
    "            for sentence in sentences if sentence.split()\n",
    "        ]\n",
    "\n",
    "        # padded_sentence_tensor = pad_sequence(sentence_embeddings, batch_first=True, padding_value=0.0)\n",
    "        # return EmbeddingDataset(padded_sentence_tensor)\n",
    "        return sentence_embeddings\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\" permet de rajouter du padding et renvoyer la taille de la séquence dans un batch\"\"\"\n",
    "    batch = [item for item in batch if len(item) > 0]  # Filtrer les séquences vides\n",
    "    lengths = torch.tensor([len(seq) for seq in batch])  # Longueurs originales\n",
    "    padded_batch = pad_sequence(batch, batch_first=True, padding_value=0.0)  # Padding des séquences\n",
    "    return padded_batch, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spice_model = SpiceEmbeddingModel()\n",
    "spice_model.load_model()\n",
    "dataset = spice_model.get_dataset(text)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 5.8416e-03, -5.5395e-03,  2.1862e-02,  ..., -8.8997e-02,\n",
      "          -3.3824e-01,  7.3939e-01],\n",
      "         [ 5.4557e-02, -1.9261e-01,  2.3487e-03,  ...,  6.7749e-02,\n",
      "          -3.6495e-02,  2.4421e-01],\n",
      "         [ 1.6232e-01,  3.9037e-02,  3.3646e-01,  ...,  3.3478e-01,\n",
      "           6.0715e-02,  3.8146e-01],\n",
      "         ...,\n",
      "         [-7.3180e-02, -9.7107e-02,  5.3287e-02,  ..., -1.1901e-01,\n",
      "           5.5130e-02,  2.1968e-01],\n",
      "         [-5.7007e-02, -7.9440e-03, -9.9562e-02,  ..., -1.4544e-01,\n",
      "          -1.2218e-01,  6.3361e-01],\n",
      "         [-6.0288e-02,  4.6746e-03,  1.5118e-01,  ...,  1.3022e-01,\n",
      "           1.3579e-01,  4.5187e-01]],\n",
      "\n",
      "        [[ 8.7352e-03, -1.8982e-01, -6.3989e-03,  ..., -1.4912e-01,\n",
      "          -1.3087e-01,  4.8510e-01],\n",
      "         [-8.3389e-02, -1.9912e-01,  2.1059e-02,  ...,  1.6368e-01,\n",
      "          -3.1491e-01,  1.3401e-01],\n",
      "         [-1.9332e-03, -3.9793e-03, -2.7759e-02,  ...,  4.0107e-01,\n",
      "           5.4105e-01,  7.1144e-01],\n",
      "         ...,\n",
      "         [-1.1303e-01, -9.0148e-02,  8.4571e-02,  ..., -3.4604e-01,\n",
      "          -6.5682e-02,  3.5861e-01],\n",
      "         [ 1.7460e-02, -6.0338e-02, -4.3307e-02,  ..., -3.3158e-01,\n",
      "          -1.1990e-02,  4.1998e-01],\n",
      "         [ 4.0957e-02, -7.7273e-02,  5.4712e-02,  ..., -9.0102e-02,\n",
      "           3.0822e-02,  2.6494e-01]],\n",
      "\n",
      "        [[-2.5388e-02, -1.4411e-03, -9.3537e-02,  ..., -8.5145e-02,\n",
      "          -5.5418e-03,  4.1404e-01],\n",
      "         [ 2.1807e-02, -1.5340e-02, -5.8193e-03,  ...,  1.1919e-01,\n",
      "          -1.9140e-01,  4.7686e-01],\n",
      "         [-5.8922e-04,  4.2220e-03,  2.1335e-02,  ..., -1.4483e-01,\n",
      "           1.9839e-01,  5.4119e-01],\n",
      "         ...,\n",
      "         [-3.5917e-01,  9.6988e-02,  7.2162e-03,  ...,  4.1879e-02,\n",
      "           1.3615e-01,  1.8828e-01],\n",
      "         [ 5.8416e-03, -5.5395e-03,  2.1862e-02,  ..., -8.8997e-02,\n",
      "          -3.3824e-01,  7.3939e-01],\n",
      "         [ 1.5019e-02, -1.7675e-02, -3.7158e-02,  ..., -4.1938e-01,\n",
      "          -1.9162e-02,  4.3523e-01]],\n",
      "\n",
      "        [[-1.1899e-02, -3.0823e-02, -3.2175e-02,  ..., -1.1002e-01,\n",
      "          -2.8573e-02,  3.3390e-01],\n",
      "         [ 1.0041e-01,  1.0723e-01, -1.6764e-02,  ..., -3.9280e-01,\n",
      "           1.1470e-02,  3.8798e-01],\n",
      "         [ 1.0971e-01, -6.9950e-02, -8.1535e-02,  ...,  5.4599e-03,\n",
      "           1.8397e-01,  3.8047e-01],\n",
      "         ...,\n",
      "         [-2.0920e-02, -7.0064e-02,  2.9552e-02,  ..., -3.9241e-01,\n",
      "          -4.7145e-02,  4.8301e-01],\n",
      "         [ 5.5368e-03,  5.6360e-03,  9.5782e-02,  ..., -6.2931e-02,\n",
      "          -8.0790e-02,  2.6287e-01],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]]])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader : \n",
    "    a, b = batch\n",
    "    # print( a.shape, b)\n",
    "    print( a)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingAndRoleDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset qui renvoie pour chaque phrase un tuple (fillers, roles)\n",
    "    - fillers : tenseur (seq_len, emb_dim) des embeddings de mots\n",
    "    - roles   : tenseur (seq_len, role_dim) des vecteurs de rôle calculés par cosinus\n",
    "    \"\"\"\n",
    "    def __init__(self, fillers_list, roles_list):\n",
    "        self.fillers_list = fillers_list\n",
    "        self.roles_list = roles_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fillers_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.fillers_list[idx], self.roles_list[idx]\n",
    "\n",
    "def positional_encoding(seq_len, role_dim, p):\n",
    "    \"\"\"\n",
    "    encoding positionnel pour les rôles un peu comme dans les transformers\n",
    "    \"\"\"\n",
    "    positions = torch.arange(seq_len, dtype=torch.float32).unsqueeze(1)  # (seq_len, 1)\n",
    "    dims = torch.arange(role_dim, dtype=torch.float32).unsqueeze(0)         # (1, role_dim)\n",
    "    pos_enc = torch.cos(2 * positions * dims / p)  # (seq_len, role_dim)\n",
    "    return pos_enc\n",
    "\n",
    "def collate_fn_fillers_roles(batch):\n",
    "    \"\"\"\n",
    "    Collate function pour un batch de tuples (fillers, roles).\n",
    "    Effectue le padding sur les fillers et les roles et renvoie aussi les longueurs.\n",
    "    \"\"\"\n",
    "    fillers_list, roles_list = zip(*batch)\n",
    "    lengths = torch.tensor([f.shape[0] for f in fillers_list], dtype=torch.long)\n",
    "    padded_fillers = pad_sequence(fillers_list, batch_first=True, padding_value=0.0)\n",
    "    padded_roles = pad_sequence(roles_list, batch_first=True, padding_value=0.0)\n",
    "    return padded_fillers, padded_roles, lengths\n",
    "\n",
    "class SpiceEmbeddingModel:\n",
    "    def __init__(self, emb_dim=50, window_size=3):\n",
    "        self.emb_dim = emb_dim\n",
    "        self.window_size = window_size\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.embeddings = None\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        # On retire les lignes contenant \":\" puis on met en minuscule et on garde uniquement les lettres et espaces\n",
    "        sentences = text.split('\\n')\n",
    "        sentences = [lst for lst in sentences if \":\" not in lst]\n",
    "        text = '\\n'.join(sentences).lower()\n",
    "        text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "        sentences = [lst for lst in text.split('\\n') if lst]\n",
    "        return sentences\n",
    "\n",
    "    def build_cooccurrence_matrix(self, words):\n",
    "        # Si le vocabulaire est vide, on le construit à partir de tous les mots du texte\n",
    "        if not self.word2idx:\n",
    "            vocab = sorted(set(words))\n",
    "            self.word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "            self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "\n",
    "        vocab_size = len(self.word2idx)\n",
    "        co_matrix = np.zeros((vocab_size, vocab_size))\n",
    "        for i, word in enumerate(words):\n",
    "            # Si le mot n'est pas dans le vocabulaire (cas d'un modèle pré-chargé avec un vocabulaire différent),\n",
    "            # on passe au mot suivant\n",
    "            if word not in self.word2idx:\n",
    "                continue\n",
    "            word_idx = self.word2idx[word]\n",
    "            start, end = max(0, i - self.window_size), min(len(words), i + self.window_size + 1)\n",
    "            for j in range(start, end):\n",
    "                # On vérifie également que le mot voisin est dans le vocabulaire\n",
    "                if i != j and words[j] in self.word2idx:\n",
    "                    neighbor_idx = self.word2idx[words[j]]\n",
    "                    co_matrix[word_idx, neighbor_idx] += 1\n",
    "        return co_matrix\n",
    "\n",
    "    def spice_embedding(self, co_matrix):\n",
    "        # On utilise une décomposition SVD pour obtenir des embeddings de dimension emb_dim\n",
    "        u, s, vt = svds(co_matrix, k=self.emb_dim)\n",
    "        embeddings = normalize(u @ np.diag(np.sqrt(s)))\n",
    "        return embeddings\n",
    "\n",
    "    def save_model(self):\n",
    "        with open(\"spice_embeddings.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.embeddings, f)\n",
    "        with open(\"spice_vocab.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.word2idx, f)\n",
    "\n",
    "    def load_model(self):\n",
    "        with open(\"spice_embeddings.pkl\", \"rb\") as f:\n",
    "            self.embeddings = pickle.load(f)\n",
    "        with open(\"spice_vocab.pkl\", \"rb\") as f:\n",
    "            self.word2idx = pickle.load(f)\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "\n",
    "    def encode_word(self, word):\n",
    "        return self.embeddings[self.word2idx[word]] if word in self.word2idx else None\n",
    "\n",
    "    def decode_embedding(self, vector, top_n=5):\n",
    "        similarities = np.dot(self.embeddings, vector)\n",
    "        closest_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "        return [self.idx2word[idx] for idx in closest_indices]\n",
    "\n",
    "    def get_dataset(self, text, role_dim=20):\n",
    "        \"\"\"\n",
    "        Pour un texte en entrée, on :\n",
    "         - pré-traite pour obtenir des phrases\n",
    "         - crée les embeddings (fillers) via SVD sur la matrice de cooccurrence\n",
    "         - pour chaque phrase, on récupère les fillers et on calcule les rôles\n",
    "           (les rôles sont calculés avec la fonction cosinus en fonction de la position)\n",
    "         - on renvoie un EmbeddingAndRoleDataset\n",
    "        \"\"\"\n",
    "        sentences = self.preprocess_text(text)\n",
    "        words = [word for sentence in sentences for word in sentence.split()]\n",
    "        co_matrix = self.build_cooccurrence_matrix(words)\n",
    "        self.embeddings = self.spice_embedding(co_matrix)\n",
    "        self.save_model()\n",
    "\n",
    "        # Déterminer la longueur maximale (pour fixer l'échelle p)\n",
    "        max_seq_len = max(len(sentence.split()) for sentence in sentences)\n",
    "\n",
    "        fillers_list = []\n",
    "        roles_list = []\n",
    "        for sentence in sentences:\n",
    "            tokens = sentence.split()\n",
    "            if not tokens:\n",
    "                continue\n",
    "            # Récupérer les fillers (embeddings) pour chaque mot de la phrase\n",
    "            sentence_fillers = []\n",
    "            for word in tokens:\n",
    "                if word in self.word2idx:\n",
    "                    emb = self.embeddings[self.word2idx[word]]\n",
    "                    sentence_fillers.append(torch.tensor(emb, dtype=torch.float32))\n",
    "            if len(sentence_fillers) == 0:\n",
    "                continue\n",
    "            sentence_fillers = torch.stack(sentence_fillers)  # (seq_len, emb_dim)\n",
    "            fillers_list.append(sentence_fillers)\n",
    "            # Calculer les rôles via un positional encoding simple\n",
    "            seq_len = sentence_fillers.size(0)\n",
    "            sentence_roles = positional_encoding(seq_len, role_dim, p=max_seq_len)\n",
    "            roles_list.append(sentence_roles)\n",
    "\n",
    "        return EmbeddingAndRoleDataset(fillers_list, roles_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spice_model = SpiceEmbeddingModel()\n",
    "spice_model.load_model()\n",
    "dataset = spice_model.get_dataset(text)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn_fillers_roles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 9, 50])\n",
      "torch.Size([4, 9, 20])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader : \n",
    "    a, b ,c = batch\n",
    "    # print( a.shape, b)\n",
    "    print( a.shape)\n",
    "    print(b.shape)\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
